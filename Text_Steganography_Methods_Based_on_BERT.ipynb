{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ub0GohbjcdAd"
      ],
      "authorship_tag": "ABX9TyMQvmvjqHzTmd0ViFwImiu0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mert-Keskin/Text-Steganography-Methods-Based-on-BERT-Character-Based-Fixed-Bit-and-Huffman-Coding-Approaches/blob/main/Text_Steganography_Methods_Based_on_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is part of my undergraduate thesis completed at Istanbul Medeniyet University, Department of Computer Engineering, and provides a detailed explanation of the methodology and results."
      ],
      "metadata": {
        "id": "HI77mKF3MPuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import hashlib\n",
        "import heapq\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "eOHEVLLpwEbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Charachter_Based_class"
      ],
      "metadata": {
        "id": "NzZxESBop6Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Character_Based_Stego:\n",
        "    def __init__(self):\n",
        "        # === Load BERT model ===\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
        "        self.model.eval()\n",
        "\n",
        "        # === Configurations ===\n",
        "        self.HEADER_BITS = 12 + 12 + 6\n",
        "        self.BLOCK_SIZE = 8\n",
        "        self.PREDICTION_SIZE = 257\n",
        "        self.HALF_WINDOW = 5\n",
        "        self.SKIP_INDEX = 0\n",
        "        self.MAX_LOOP_INDEX = 3\n",
        "        self.skip_counter = 0\n",
        "        self.preds_index_counter = 0\n",
        "\n",
        "    def random_hash(self, i, seed, mod):\n",
        "        input_str = f\"{seed}_{i}\".encode()\n",
        "        hash_bytes = hashlib.sha256(input_str).digest()\n",
        "        hash_int = int.from_bytes(hash_bytes, 'big')\n",
        "        return hash_int % mod\n",
        "\n",
        "    def char_to_bin(self, c):\n",
        "        return format(ord(c), f\"0{self.BLOCK_SIZE}b\")\n",
        "\n",
        "    def bin_to_char(self, b):\n",
        "        return chr(int(b, 2))\n",
        "\n",
        "    def int_to_bin(self, n, bits):\n",
        "        return format(n, f\"0{bits}b\")\n",
        "\n",
        "    def bin_to_int(self, b):\n",
        "        return int(b, 2)\n",
        "\n",
        "    def get_prediction_list(self, masked_text):\n",
        "        tokens = self.tokenizer.tokenize(masked_text)\n",
        "        indexed = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask_index = tokens.index('[MASK]')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_ids = torch.tensor([indexed])\n",
        "            outputs = self.model(input_ids)\n",
        "            predictions = outputs.logits[0, mask_index]\n",
        "            top_preds = torch.topk(predictions, self.PREDICTION_SIZE).indices.tolist()\n",
        "            return self.tokenizer.convert_ids_to_tokens(top_preds)\n",
        "\n",
        "    def create_window(self, words, index):\n",
        "        start = max(0, index - self.HALF_WINDOW)\n",
        "        end = min(len(words), index + self.HALF_WINDOW + 1)\n",
        "        return words[start:end], index - start\n",
        "\n",
        "    def encode_header(self, seed, char_count, loop_index):\n",
        "        return self.int_to_bin(seed, 12) + self.int_to_bin(char_count, 12) + self.int_to_bin(loop_index, 6)\n",
        "\n",
        "    def decode_header(self, bin_data):\n",
        "        seed = self.bin_to_int(bin_data[0:12])\n",
        "        count = self.bin_to_int(bin_data[12:24])\n",
        "        loop_index = self.bin_to_int(bin_data[24:30])\n",
        "        return seed, count, loop_index\n",
        "\n",
        "    def hide_bits_in_text(self, words, binary_data, start_idx):\n",
        "        stego_words = words[:]\n",
        "        for i in range(0, len(binary_data), 8):\n",
        "            block = binary_data[i:i+8].ljust(8, '0')\n",
        "            index = start_idx + (i // 8) * (self.HALF_WINDOW + 1)\n",
        "            if index >= len(words):\n",
        "                break\n",
        "            window, mask_pos = self.create_window(stego_words, index)\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            masked_text = ' '.join(window)\n",
        "            preds = self.get_prediction_list(masked_text)\n",
        "            pred_index = int(block, 2)\n",
        "            chosen_word = preds[pred_index] if pred_index < len(preds) else preds[self.SKIP_INDEX]\n",
        "            print(index)\n",
        "            stego_words[index] = chosen_word\n",
        "            print(\"header words: {}\".format(chosen_word))\n",
        "        print(stego_words)\n",
        "        return stego_words\n",
        "\n",
        "    def hide_message(self, cover_text, secret_msg, seed, loop_index):\n",
        "        words = cover_text.split()\n",
        "        binary_message = ''.join([self.char_to_bin(c) for c in secret_msg])\n",
        "        header_bin = self.encode_header(seed, len(secret_msg), loop_index)\n",
        "\n",
        "        header_block_count = (len(header_bin) + 7) // 8\n",
        "        reserved_range_end = self.HALF_WINDOW + header_block_count * (self.HALF_WINDOW + 1)\n",
        "        print(\"reversed: {}\".format(reserved_range_end))\n",
        "\n",
        "        print(\"[ENCODE] Header binary:\", header_bin)\n",
        "        print(f\"[ENCODE] seed={seed}, char_count={len(secret_msg)}, loop_index={loop_index}\")\n",
        "\n",
        "        stego = self.hide_bits_in_text(words, header_bin, start_idx=self.HALF_WINDOW + 1)\n",
        "\n",
        "        current_loop_index = loop_index\n",
        "        used_indices = set()\n",
        "        hidden = 0\n",
        "        i = 0\n",
        "        self.preds_index_counter = 0\n",
        "        self.skip_counter = 0\n",
        "\n",
        "        while hidden < len(secret_msg):\n",
        "            idx = self.random_hash(i, seed, len(words))\n",
        "            if idx <= reserved_range_end or any(abs(idx - u) <= self.HALF_WINDOW for u in used_indices):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if idx < len(words):\n",
        "                window, mask_pos = self.create_window(stego, idx)\n",
        "                window[mask_pos] = '[MASK]'\n",
        "                masked_text = ' '.join(window)\n",
        "                preds = self.get_prediction_list(masked_text)\n",
        "                letter = secret_msg[hidden]\n",
        "                print(f\"\\n🔐 Embedding character '{letter}' at index {idx}\")\n",
        "                print(f\"Top predictions: {preds[:5]}\")\n",
        "                found = False\n",
        "                for j in range(1, len(preds)):\n",
        "                    if len(preds[j]) > current_loop_index and preds[j][current_loop_index] == letter:\n",
        "                        self.preds_index_counter += j\n",
        "                        print(f\"✅ Match at position {j}: '{preds[j]}'\")\n",
        "                        stego[idx] = preds[j]\n",
        "                        found = True\n",
        "                        break\n",
        "\n",
        "                if found:\n",
        "                    hidden += 1\n",
        "                else:\n",
        "                    self.skip_counter += 1\n",
        "                    print(\"skip index chosen: {}\".format(preds[self.SKIP_INDEX]))\n",
        "                    stego[idx] = preds[self.SKIP_INDEX]\n",
        "            i += 1\n",
        "            used_indices.add(idx)\n",
        "            current_loop_index = (current_loop_index + 1) % self.MAX_LOOP_INDEX\n",
        "        print(\"total travel words: {}\".format(len(used_indices)))\n",
        "        print(\"total travel words with header: {}\".format(len(used_indices) + 4))\n",
        "        print(\"skipped: {}\".format(self.skip_counter))\n",
        "        print(\"avarage {}\".format(self.preds_index_counter / (len(used_indices) - self.skip_counter)))\n",
        "        return ' '.join(stego), len(used_indices) + 4 - self.skip_counter, self.skip_counter\n",
        "\n",
        "    def extract_bits_from_text(self, words, bit_count, start_idx):\n",
        "        bits = ''\n",
        "        for i in range(0, bit_count, 8):\n",
        "            index = start_idx + (i // 8) * (self.HALF_WINDOW + 1)\n",
        "            if index >= len(words):\n",
        "                break\n",
        "            window, mask_pos = self.create_window(words, index)\n",
        "            original_word = words[index]\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            masked_text = ' '.join(window)\n",
        "            preds = self.get_prediction_list(masked_text)\n",
        "            try:\n",
        "                if original_word not in preds:\n",
        "                    print(f\"  ⚠️ '{original_word}' NOT found in predictions.\")\n",
        "                b = self.int_to_bin(preds.index(original_word), 8)\n",
        "            except ValueError:\n",
        "                b = self.int_to_bin(0, 8)\n",
        "            bits += b\n",
        "        return bits\n",
        "\n",
        "    def extract_message(self, stego_text):\n",
        "        words = stego_text.split()\n",
        "        header_bits = self.extract_bits_from_text(words, self.HEADER_BITS, start_idx=self.HALF_WINDOW + 1)\n",
        "        print(\"[DECODE] Extracted header binary:\", header_bits)\n",
        "\n",
        "        seed, count, loop_index = self.decode_header(header_bits)\n",
        "        print(f\"[DECODE] Decoded header: seed={seed}, char_count={count}, loop_index={loop_index}\")\n",
        "\n",
        "        extracted = ''\n",
        "        current_loop_index = loop_index\n",
        "        used_indices = set()\n",
        "        i = 0\n",
        "        found = 0\n",
        "\n",
        "        while found < count:\n",
        "            idx = self.random_hash(i, seed, len(words))\n",
        "            header_block_count = (self.HEADER_BITS + 7) // 8\n",
        "            reserved_range_end = self.HALF_WINDOW + header_block_count * (self.HALF_WINDOW + 1)\n",
        "\n",
        "            if idx <= reserved_range_end or any(abs(idx - u) <= self.HALF_WINDOW for u in used_indices) or idx >= len(words):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            original_word = words[idx]\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            masked_text = ' '.join(window)\n",
        "            preds = self.get_prediction_list(masked_text)\n",
        "            print(\"-------------------\")\n",
        "            print(masked_text)\n",
        "            print(f\"Top predictions: {preds[:5]}\")\n",
        "            print(\"-------------------\")\n",
        "\n",
        "            if original_word == preds[self.SKIP_INDEX]:\n",
        "                print(\"came across a skip word: {}\".format(preds[self.SKIP_INDEX]))\n",
        "                i += 1\n",
        "                current_loop_index = (current_loop_index + 1) % self.MAX_LOOP_INDEX\n",
        "                used_indices.add(idx)\n",
        "                continue\n",
        "\n",
        "            if len(original_word) > current_loop_index:\n",
        "                extracted += original_word[current_loop_index]\n",
        "                print(\"✅ original word: {}  letter: {}\".format(original_word, original_word[current_loop_index]))\n",
        "                found += 1\n",
        "\n",
        "            i += 1\n",
        "            used_indices.add(idx)\n",
        "            current_loop_index = (current_loop_index + 1) % self.MAX_LOOP_INDEX\n",
        "\n",
        "        print(\"total traveled words: {}\".format(len(used_indices)))\n",
        "        print(\"total traveled words with header: {}\".format(len(used_indices) + 4))\n",
        "        print(\"skipped: {}\".format(self.skip_counter))\n",
        "        print(\"avarage {}\".format(self.preds_index_counter / (len(used_indices) - self.skip_counter)))\n",
        "        return extracted\n"
      ],
      "metadata": {
        "id": "-VAMrajtp-ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flexible Fixed Bit"
      ],
      "metadata": {
        "id": "xFMvFEF7-Jer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Fixed_Bit_Stego:\n",
        "    def __init__(self, bit_width=2):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
        "        self.model.eval()\n",
        "\n",
        "        self.BLOCK_SIZE = 8\n",
        "        self.HEADER_BITS = 24\n",
        "        self.HALF_WINDOW = 5\n",
        "        self.bit_width = bit_width\n",
        "        self.message_changes = 0\n",
        "        self.total_changes = 0\n",
        "        self.header_changes = 0\n",
        "\n",
        "    def squarehash(self, i, seed, mod):\n",
        "        input_str = f\"{seed}_{i}\".encode()\n",
        "        hash_bytes = hashlib.sha256(input_str).digest()\n",
        "        hash_int = int.from_bytes(hash_bytes, 'big')\n",
        "        return hash_int % mod\n",
        "\n",
        "    def int_to_bin(self, n, bits):\n",
        "        return format(n, f\"0{bits}b\")\n",
        "\n",
        "    def bin_to_int(self, b):\n",
        "        return int(b, 2)\n",
        "\n",
        "    def text_to_bits(self, text):\n",
        "        return ''.join(format(ord(c), '08b') for c in text)\n",
        "\n",
        "    def bits_to_text(self, bits):\n",
        "        chars = [chr(int(bits[i:i+8], 2)) for i in range(0, len(bits), 8)]\n",
        "        return ''.join(chars)\n",
        "\n",
        "    def get_predictions(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        indexed = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask_index = tokens.index('[MASK]')\n",
        "        with torch.no_grad():\n",
        "            input_ids = torch.tensor([indexed])\n",
        "            outputs = self.model(input_ids)\n",
        "            logits = outputs.logits[0, mask_index]\n",
        "            top_preds = torch.topk(logits, 2**self.bit_width).indices.tolist()\n",
        "            return self.tokenizer.convert_ids_to_tokens(top_preds)\n",
        "\n",
        "    def create_window(self, words, idx):\n",
        "        start = max(0, idx - self.HALF_WINDOW)\n",
        "        end = min(len(words), idx + self.HALF_WINDOW + 1)\n",
        "        return words[start:end], idx - start\n",
        "\n",
        "    def encode_header(self, seed, bit_len):\n",
        "        return self.int_to_bin(seed, 12) + self.int_to_bin(bit_len, 12)\n",
        "\n",
        "    def decode_header(self, bits):\n",
        "        return self.bin_to_int(bits[:12]), self.bin_to_int(bits[12:24])\n",
        "\n",
        "    def hide_header(self, words, seed, bit_len):\n",
        "        header_bin = self.encode_header(seed, bit_len)\n",
        "        changed = 0\n",
        "        for i in range(0, len(header_bin), self.bit_width):\n",
        "            block = header_bin[i:i+self.bit_width].ljust(self.bit_width, '0')\n",
        "            idx = self.HALF_WINDOW + 1 + (i // self.bit_width) * (self.HALF_WINDOW + 1)\n",
        "            print(f\"[HEADER] Index: {idx}, Bits: {block}\")\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds = self.get_predictions(' '.join(window))\n",
        "            value = int(block, 2)\n",
        "            selected = preds[value] if value < len(preds) else preds[0]\n",
        "            print(f\"  Prediction list: {preds}\")\n",
        "            print(f\"  Selected word: {selected}\")\n",
        "            words[idx] = selected\n",
        "            changed += 1\n",
        "        print(f\"[HEADER] Total header words changed: {changed}\")\n",
        "        return words, changed\n",
        "\n",
        "    def hide_message(self, cover_text, secret_msg, seed):\n",
        "        words = cover_text.split()\n",
        "        bits = self.text_to_bits(secret_msg)\n",
        "        words, self.header_changes = self.hide_header(words, seed, len(bits))\n",
        "        used = set()\n",
        "\n",
        "        header_block_count = self.HEADER_BITS // self.bit_width\n",
        "        reserved_range_end = self.HALF_WINDOW + header_block_count * (self.HALF_WINDOW + 1)\n",
        "        print(\"[INFO] Reserved header region ends at:\", reserved_range_end)\n",
        "\n",
        "        i = 0\n",
        "        hidden = 0\n",
        "        while hidden < len(bits):\n",
        "            idx = self.squarehash(i, seed, len(words))\n",
        "\n",
        "            if idx <= reserved_range_end or any(abs(idx - u) <= self.HALF_WINDOW for u in used):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            block = bits[hidden:hidden+self.bit_width].ljust(self.bit_width, '0')\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds = self.get_predictions(' '.join(window))\n",
        "            val = int(block, 2)\n",
        "            selected = preds[val] if val < len(preds) else preds[0]\n",
        "            print(f\"[MESSAGE] Index: {idx}, Bits: {block}\")\n",
        "            print(f\"  Prediction list: {preds}\")\n",
        "            print(f\"  Selected word: {selected}\")\n",
        "            words[idx] = selected\n",
        "            hidden += self.bit_width\n",
        "            used.add(idx)\n",
        "            self.message_changes += 1\n",
        "            i += 1\n",
        "        self.total_changes = self.header_changes + self.message_changes\n",
        "        print(f\"[INFO] Total message words changed: {self.message_changes}\")\n",
        "        print(f\"[INFO] Total words changed: {self.total_changes}\")\n",
        "        return ' '.join(words), self.total_changes\n",
        "\n",
        "    def extract_header(self, words):\n",
        "        bits = ''\n",
        "        for i in range(self.HEADER_BITS // self.bit_width):\n",
        "            idx = self.HALF_WINDOW + 1 + i * (self.HALF_WINDOW + 1)\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            original = words[idx]\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds = self.get_predictions(' '.join(window))\n",
        "            if original in preds:\n",
        "                bits += self.int_to_bin(preds.index(original), self.bit_width)\n",
        "            else:\n",
        "                bits += '0' * self.bit_width\n",
        "        return self.decode_header(bits)\n",
        "\n",
        "    def extract_message(self, stego_text):\n",
        "        words = stego_text.split()\n",
        "        seed, bit_len = self.extract_header(words)\n",
        "        print(seed)\n",
        "        print(bit_len)\n",
        "        bits = ''\n",
        "\n",
        "        header_block_count = self.HEADER_BITS // self.bit_width\n",
        "        reserved_range_end = self.HALF_WINDOW + header_block_count * (self.HALF_WINDOW + 1)\n",
        "        print(\"reserved range: {}\".format(reserved_range_end))\n",
        "\n",
        "        i = 0\n",
        "        used = set()\n",
        "        while len(bits) < bit_len:\n",
        "            idx = self.squarehash(i, seed, len(words))\n",
        "\n",
        "            if idx <= reserved_range_end or any(abs(idx - u) <= self.HALF_WINDOW for u in used):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            original = words[idx]\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds = self.get_predictions(' '.join(window))\n",
        "            if original in preds:\n",
        "                bits += self.int_to_bin(preds.index(original), self.bit_width)\n",
        "            else:\n",
        "                print(\"Error!\")\n",
        "                bits += '0' * self.bit_width\n",
        "            used.add(idx)\n",
        "            i += 1\n",
        "        print(f\"[INFO] Total message words changed: {self.message_changes}\")\n",
        "        print(f\"[INFO] Total words changed: {self.total_changes}\")\n",
        "        return self.bits_to_text(bits[:bit_len])\n"
      ],
      "metadata": {
        "id": "LZu4oA4G-I60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Huffman with class"
      ],
      "metadata": {
        "id": "CW0KNHjO43M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Huffman_Stego:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
        "        self.model.eval()\n",
        "\n",
        "        self.BLOCK_SIZE = 8\n",
        "        self.HEADER_BITS = 24\n",
        "        self.HALF_WINDOW = 5\n",
        "        self.STRIDE = 2 * self.HALF_WINDOW + 1\n",
        "        self.TOP_K = 16\n",
        "        self.changes = 0\n",
        "        self.header_changes = 0\n",
        "        self.message_changes = 0\n",
        "        self.total_changes = 0\n",
        "\n",
        "    def random_hash(self, i, seed, mod):\n",
        "        input_str = f\"{seed}_{i}\".encode()\n",
        "        hash_bytes = hashlib.sha256(input_str).digest()\n",
        "        hash_int = int.from_bytes(hash_bytes, 'big')\n",
        "        return hash_int % mod\n",
        "\n",
        "    def int_to_bin(self, n, bits):\n",
        "        return format(n, f\"0{bits}b\")\n",
        "\n",
        "    def bin_to_int(self, b):\n",
        "        return int(b, 2)\n",
        "\n",
        "    def text_to_bits(self, text):\n",
        "        return ''.join(format(ord(c), '08b') for c in text)\n",
        "\n",
        "    def bits_to_text(self, bits):\n",
        "        chars = [chr(int(bits[i:i+8], 2)) for i in range(0, len(bits), 8)]\n",
        "        return ''.join(chars)\n",
        "\n",
        "    def get_predictions(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        indexed = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        mask_index = tokens.index('[MASK]')\n",
        "        with torch.no_grad():\n",
        "            input_ids = torch.tensor([indexed])\n",
        "            outputs = self.model(input_ids)\n",
        "            logits = outputs.logits[0, mask_index]\n",
        "            probs = F.softmax(logits, dim=0)\n",
        "            top_probs, top_indices = torch.topk(probs, self.TOP_K)\n",
        "            return self.tokenizer.convert_ids_to_tokens(top_indices.tolist()), top_probs.tolist()\n",
        "\n",
        "    def create_window(self, words, idx):\n",
        "        start = max(0, idx - self.HALF_WINDOW)\n",
        "        end = min(len(words), idx + self.HALF_WINDOW + 1)\n",
        "        return words[start:end], idx - start\n",
        "\n",
        "    class Node:\n",
        "        def __init__(self, word, prob):\n",
        "            self.word = word\n",
        "            self.prob = prob\n",
        "            self.left = None\n",
        "            self.right = None\n",
        "\n",
        "        def __lt__(self, other):\n",
        "            return self.prob < other.prob\n",
        "\n",
        "    def build_huffman(self, words, probs):\n",
        "        heap = [self.Node(w, p) for w, p in zip(words, probs)]\n",
        "        heapq.heapify(heap)\n",
        "        while len(heap) > 1:\n",
        "            n1 = heapq.heappop(heap)\n",
        "            n2 = heapq.heappop(heap)\n",
        "            merged = self.Node(None, n1.prob + n2.prob)\n",
        "            merged.left = n1\n",
        "            merged.right = n2\n",
        "            heapq.heappush(heap, merged)\n",
        "        root = heap[0]\n",
        "        encoding = {}\n",
        "\n",
        "        def traverse(node, path):\n",
        "            if node.word is not None:\n",
        "                encoding[node.word] = path\n",
        "            if node.left:\n",
        "                traverse(node.left, path + '0')\n",
        "            if node.right:\n",
        "                traverse(node.right, path + '1')\n",
        "\n",
        "        traverse(root, '')\n",
        "        return encoding\n",
        "\n",
        "    def encode_header(self, seed, bit_len):\n",
        "        return self.int_to_bin(seed, 12) + self.int_to_bin(bit_len, 12)\n",
        "\n",
        "    def decode_header(self, bits):\n",
        "        return self.bin_to_int(bits[:12]), self.bin_to_int(bits[12:24])\n",
        "\n",
        "    def hide_header(self, words, seed, bit_len):\n",
        "        header_bin = self.encode_header(seed, bit_len) + '0' * 0\n",
        "        print(\"Bits: {}\".format(header_bin))\n",
        "        hidden = 0\n",
        "        i = 0\n",
        "        self.changes = 0\n",
        "        len_header_bin = len(header_bin)\n",
        "        while hidden < len_header_bin:\n",
        "            idx = self.HALF_WINDOW+1 + (i * (self.HALF_WINDOW+1))\n",
        "            if idx >= len(words):\n",
        "                break\n",
        "\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds, probs = self.get_predictions(' '.join(window))\n",
        "            huff = self.build_huffman(preds, probs)\n",
        "\n",
        "            print(f\"\\n🔐 Hiding Header Block at index {idx}\")\n",
        "            print(f\"Header bits remaining: {header_bin[hidden:hidden+10]}\")\n",
        "            print(f\"Huffman codes: {huff}\")\n",
        "\n",
        "            matched = False\n",
        "            for w, code in huff.items():\n",
        "                remaining = len(header_bin) - hidden\n",
        "                if len(code) > remaining:\n",
        "                    header_bin += '0' * 4\n",
        "                    print(\"Came to an end and padded.\")\n",
        "                if header_bin.startswith(code, hidden):\n",
        "                    words[idx] = w\n",
        "                    print(f\"✅ Selected word '{w}' for code '{code}'\")\n",
        "                    hidden += len(code)\n",
        "                    matched = True\n",
        "                    self.changes += 1\n",
        "                    break\n",
        "            if not matched:\n",
        "                print(f\"⚠️ No exact match.\")\n",
        "            i += 1\n",
        "\n",
        "        print(f\"[INFO] Total header words changed: {self.changes}\")\n",
        "        return words, self.changes\n",
        "\n",
        "    def hide_message(self, cover_text, secret_msg, seed):\n",
        "        words = cover_text.split()\n",
        "        bits = self.text_to_bits(secret_msg) + '0' * 0\n",
        "        words, self.header_changes = self.hide_header(words, seed, len(bits)-0)\n",
        "        used = set()\n",
        "        i = 0\n",
        "        hidden = 0\n",
        "        self.message_changes = 0\n",
        "        len_bits = len(bits)\n",
        "        while hidden < len_bits:\n",
        "            idx = self.random_hash(i, seed, len(words))\n",
        "            if idx < (self.HEADER_BITS // 2 * (self.HALF_WINDOW+1)) or any(abs(idx - u) <= self.HALF_WINDOW for u in used):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds, probs = self.get_predictions(' '.join(window))\n",
        "            huff = self.build_huffman(preds, probs)\n",
        "\n",
        "            print(f\"\\n📦 Hiding Secret Message Block at index {idx}\")\n",
        "            print(f\"Bits remaining: {bits[hidden:hidden+10]}\")\n",
        "            print(f\"Huffman codes: {huff}\")\n",
        "\n",
        "            matched = False\n",
        "            for w, code in huff.items():\n",
        "                remaining = len(bits) - hidden\n",
        "                if len(code) > remaining:\n",
        "                    bits+= '0'*4\n",
        "                    print(\"Came to an end and padded.\")\n",
        "                if bits.startswith(code, hidden):\n",
        "                    words[idx] = w\n",
        "                    print(f\"✅ Selected word '{w}' for code '{code}'\")\n",
        "                    hidden += len(code)\n",
        "                    matched = True\n",
        "                    self.message_changes += 1\n",
        "                    break\n",
        "            if not matched:\n",
        "                print(f\"⚠️ No match.\")\n",
        "            used.add(idx)\n",
        "            i += 1\n",
        "        self.total_changes = self.header_changes + self.message_changes\n",
        "        return ' '.join(words), self.total_changes\n",
        "\n",
        "    def extract_header(self, words):\n",
        "        bits = ''\n",
        "        i = 0\n",
        "\n",
        "        while len(bits) < self.HEADER_BITS:\n",
        "            idx = self.HALF_WINDOW+1 + (i * (self.HALF_WINDOW+1))\n",
        "            if idx >= len(words):\n",
        "                break\n",
        "\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            original = words[idx]\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds, probs = self.get_predictions(' '.join(window))\n",
        "            huff = self.build_huffman(preds, probs)\n",
        "\n",
        "            print(f\"\\n🧩 Extracting Header at index {idx}\")\n",
        "            print(f\"Original word: {original}\")\n",
        "            print(f\"Huffman codes: {huff}\")\n",
        "\n",
        "            if original in huff:\n",
        "                bits += huff[original]\n",
        "            else:\n",
        "                bits += '00'\n",
        "                print(f\"⚠️ '{original}' not found, defaulting to '00'\")\n",
        "\n",
        "            i += 1\n",
        "        return self.decode_header(bits)\n",
        "\n",
        "    def extract_message(self, stego_text):\n",
        "        words = stego_text.split()\n",
        "        seed, bit_len = self.extract_header(words)\n",
        "        print(f\"\\n[HEADER DECODED] seed={seed}, bit_len={bit_len}\\n\")\n",
        "\n",
        "        bits = ''\n",
        "        i = 0\n",
        "        used = set()\n",
        "\n",
        "        while len(bits) < bit_len:\n",
        "            idx = self.random_hash(i, seed, len(words))\n",
        "\n",
        "            if idx < (self.HEADER_BITS // 2 * (self.HALF_WINDOW+1)) or any(abs(idx - u) <= self.HALF_WINDOW for u in used):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            window, mask_pos = self.create_window(words, idx)\n",
        "            original = words[idx]\n",
        "            window[mask_pos] = '[MASK]'\n",
        "            preds, probs = self.get_predictions(' '.join(window))\n",
        "            huff = self.build_huffman(preds, probs)\n",
        "\n",
        "            print(f\"\\n🧩 Extracting Message at index {idx}\")\n",
        "            print(f\"Original word: {original}\")\n",
        "            print(f\"Huffman codes: {huff}\")\n",
        "\n",
        "            if original in huff:\n",
        "                bits += huff[original]\n",
        "            else:\n",
        "                bits += '00'\n",
        "                print(f\"⚠️ '{original}' not found, defaulting to '00'\")\n",
        "            used.add(idx)\n",
        "            i += 1\n",
        "        print(f\"\\n[INFO] Total message words changed: {self.message_changes}\")\n",
        "        print(f\"[INFO] Total words changed (header + message): {self.total_changes}\")\n",
        "        return self.bits_to_text(bits[:bit_len])\n"
      ],
      "metadata": {
        "id": "lp_A7Bfu47QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Huffman"
      ],
      "metadata": {
        "id": "ub0GohbjcdAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cover_text = \"\"\"What is steganography?\n",
        "Steganography is the technique of hiding data within an ordinary, nonsecret file or message to avoid detection; the hidden data is then extracted at its destination. Steganography use can be combined with encryption as an extra step for hiding or protecting data. The word steganography is derived from the Greek word steganos, meaning \"hidden or covered,\" and the Greek root graph, meaning \"to write.\"\n",
        "\n",
        "Steganography can be used to conceal almost any type of digital content, including text, image, video or audio content. The secret data can be hidden inside almost any other type of digital content. The content to be concealed through steganography -- called hidden text -- is often encrypted before being incorporated into the innocuous-seeming cover text file or data stream. If not encrypted, the hidden text is commonly processed in some method to increase the difficulty of detecting the secret content.\n",
        "\n",
        "What are some examples of steganography?\n",
        "Steganography is practiced by those wishing to convey a secret message or code. While there are many legitimate uses for steganography, some malware developers use steganography to obscure the transmission of malicious code -- known as stegware.\n",
        "\n",
        "Forms of steganography have been used for centuries and include almost any technique for hiding a secret message in an otherwise harmless container. For example, using invisible ink to hide secret messages in otherwise inoffensive messages; hiding documents recorded on microdot, which can be as small as 1 millimeter in diameter; hiding messages on or inside legitimate-seeming correspondence; and even using multiplayer gaming environments to share information.\n",
        "\"\"\"\n",
        "secret_text = \"merhaba\"\n",
        "huff = Huffman_Stego()"
      ],
      "metadata": {
        "id": "6FKzKCmwaKx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stego, changed = huff.hide_message(cover_text, secret_text, seed=1234)\n"
      ],
      "metadata": {
        "id": "o_Z9hLrhaJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = huff.extract_message(stego)"
      ],
      "metadata": {
        "id": "11QMGvX7cBOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hidden)"
      ],
      "metadata": {
        "id": "m0WjKTaebvH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "WVaAZ4UPn1bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch scikit-learn --quiet\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# === 1. Embedding Rate (ER) ===\n",
        "def compute_er(num_bits, total_words):\n",
        "    return num_bits / total_words\n",
        "\n",
        "# === 2. Kullback-Leibler Divergence (KLD) ===\n",
        "def compute_kld(text_p, text_q):\n",
        "    vectorizer = CountVectorizer()\n",
        "    X = vectorizer.fit_transform([text_p, text_q]).toarray().astype(float)\n",
        "    P, Q = X[0], X[1]\n",
        "    P /= P.sum()\n",
        "    Q /= Q.sum()\n",
        "    epsilon = 1e-10\n",
        "    P = np.clip(P, epsilon, 1)\n",
        "    Q = np.clip(Q, epsilon, 1)\n",
        "    return np.sum(P * np.log(P / Q))\n",
        "\n",
        "# === 3. Perplexity (PPL) using GPT-2 ===\n",
        "\n",
        "def compute_ppl(text, max_length=1024):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    model.eval()\n",
        "\n",
        "    tokens = tokenizer.encode(text)\n",
        "    if len(tokens) <= max_length:\n",
        "        input_ids = torch.tensor([tokens])\n",
        "        with torch.no_grad():\n",
        "            loss = model(input_ids, labels=input_ids).loss\n",
        "            return torch.exp(loss).item()\n",
        "\n",
        "    # Split long text into chunks\n",
        "    stride = 512\n",
        "    losses = []\n",
        "    for i in range(0, len(tokens) - 1, stride):\n",
        "        input_ids = torch.tensor([tokens[i:i + max_length]])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            losses.append(outputs.loss.item())\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    return math.exp(mean_loss)\n",
        "\n",
        "# === 4. Semantic Similarity (SIM) ===\n",
        "def compute_similarity(text1, text2):\n",
        "    vectorizer = CountVectorizer().fit([text1, text2])\n",
        "    vectors = vectorizer.transform([text1, text2]).toarray()\n",
        "    return cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n"
      ],
      "metadata": {
        "id": "O4OZ7dS8R8O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Multiple Samples"
      ],
      "metadata": {
        "id": "o0toUdaADnzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"secret_message.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    secret_messages = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "with open(\"cover_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    cover_text = f.read()"
      ],
      "metadata": {
        "id": "ZeE9pY15A6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cover_text = cover_text"
      ],
      "metadata": {
        "id": "D8opSBSibRuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accumulated_results = {algo: [] for algo in ['Character', 'Fixed_Bit_2', 'Fixed_Bit_4', 'Huffman']}\n",
        "\n",
        "for secret_text in secret_messages:\n",
        "    for StegoBuilder, name in [\n",
        "        (lambda: Character_Based_Stego(), \"Character\"),\n",
        "        (lambda: Fixed_Bit_Stego(bit_width=2), \"Fixed_Bit_2\"),\n",
        "        (lambda: Fixed_Bit_Stego(bit_width=4), \"Fixed_Bit_4\"),\n",
        "        (lambda: Huffman_Stego(), \"Huffman\")\n",
        "    ]:\n",
        "        algo = StegoBuilder()\n",
        "        if name == \"Character\":\n",
        "            stego, changed, skipped = algo.hide_message(cover_text, secret_text, seed=1234, loop_index=0)\n",
        "            header_size = 32\n",
        "        else:\n",
        "            stego, changed = algo.hide_message(cover_text, secret_text, seed=1234)\n",
        "            header_size = 24\n",
        "            skipped = 0\n",
        "\n",
        "        extracted = algo.extract_message(stego)\n",
        "\n",
        "\n",
        "        ppl_cover = compute_ppl(cover_text)\n",
        "        ppl_stego = compute_ppl(stego)\n",
        "        kld_result = compute_kld(cover_text, stego)\n",
        "        sim_result = compute_similarity(cover_text, stego)\n",
        "        metrics = {\n",
        "            'ChangedWords': changed,\n",
        "            'SkippedWords': skipped,\n",
        "            'EmbeddingRate': compute_er(len(secret_text) * 8+header_size, changed),\n",
        "            'KLD': kld_result,\n",
        "            'PPL_Cover': ppl_cover,\n",
        "            'PPL_Stego': ppl_stego,\n",
        "            'SemanticSim': sim_result,\n",
        "            'PPL_per_word': (ppl_stego - ppl_cover) / changed if changed else 0,\n",
        "            'KLD_per_word': kld_result / changed if changed else 0,\n",
        "            'SIM_loss_per_word': (1 - sim_result) / changed if changed else 0,\n",
        "            'Success': extracted == secret_text\n",
        "        }\n",
        "        accumulated_results[name].append(metrics)\n"
      ],
      "metadata": {
        "id": "vIXgmQiJBWGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "final_results = []\n",
        "\n",
        "for algo_name, runs in accumulated_results.items():\n",
        "    avg_result = {\n",
        "        'Algorithm': algo_name,\n",
        "        'AvgChangedWords': np.mean([r['ChangedWords'] for r in runs]),\n",
        "        'SkippedWords': np.mean([r['SkippedWords'] for r in runs]),\n",
        "        'AvgEmbeddingRate': np.mean([r['EmbeddingRate'] for r in runs]),\n",
        "        'AvgKLD': np.mean([r['KLD'] for r in runs]),\n",
        "        'AvgPPL_Cover': np.mean([r['PPL_Cover'] for r in runs]),\n",
        "        'AvgPPL_Stego': np.mean([r['PPL_Stego'] for r in runs]),\n",
        "        'AvgSemanticSim': np.mean([r['SemanticSim'] for r in runs]),\n",
        "        'AvgPPL_PerWord': np.mean([r['PPL_per_word'] for r in runs]),\n",
        "        'AvgKLD_PerWord': np.mean([r['KLD_per_word'] for r in runs]),\n",
        "        'AvgSIM_Loss_PerWord': np.mean([r['SIM_loss_per_word'] for r in runs]),\n",
        "        'SuccessRate': np.mean([r['Success'] for r in runs]),\n",
        "        'Runs': len(runs)\n",
        "    }\n",
        "    final_results.append(avg_result)\n"
      ],
      "metadata": {
        "id": "ZCchVfudBOr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"evaluation_averages.csv\", \"w\", newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=final_results[0].keys())\n",
        "    writer.writeheader()\n",
        "    writer.writerows(final_results)\n"
      ],
      "metadata": {
        "id": "DQmcvZOrBr83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}